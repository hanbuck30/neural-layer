{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTaGbGTlsRIlr6ZikadC1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanbuck30/neural-layer/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "URCoh7ZY4l0G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 5 # 시점의 수. NLP에서는 보통 문장의 길이\n",
        "input_size = 3 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량\n",
        "output_size=5\n",
        "batch_size=1\n",
        "inputs = torch.randn(batch_size,timesteps,input_size)\n",
        "hidden_state_t=torch.zeros(hidden_size,requires_grad=True)\n",
        "num_layers=2\n"
      ],
      "metadata": {
        "id": "CTkE6oG449h9"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = torch.randn(hidden_size, input_size,requires_grad=True) \n",
        "Wh = torch.randn(hidden_size, hidden_size,requires_grad=True)\n",
        "b = torch.randn(hidden_size,requires_grad=True)\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "jQ5UYob-61bi"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Wx.shape)\n",
        "print(Wh.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0vcWbbQ7XW2",
        "outputId": "797aca8f-4f41-4929-f0da-8bf05bb99103"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3])\n",
            "torch.Size([8, 8])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJB-9u6W3aOJ",
        "outputId": "22c63bb7-ab2c-44fd-fd6d-4a8d2070bc88"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y=torch.LongTensor([[0,1,2,3,4]])\n",
        "y_data=[Y]\n",
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srZJhB-ck4ev",
        "outputId": "cd432e4c-a7fd-4251-e1cc-3c9d459d7d80"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_fc=torch.randn(output_size,requires_grad=True)\n",
        "w_fc=torch.randn(hidden_size, output_size,requires_grad=True)"
      ],
      "metadata": {
        "id": "y5usjXzofPhm"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_state=[]\n",
        "total_hidden_states=[]\n",
        "def RNNmodel(inputs,hidden_state_t,num_layers):\n",
        "    def RNN(inputs,hidden_state_t,num_layers):\n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                total_hidden_states.append(output_t) \n",
        "                hidden_state_t = output_t\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def RNN_two(inputs,hidden_state_t,num_layers):\n",
        "        for k in range(1,num_layers):\n",
        "            globals()['W{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            globals()['b{}'.format(k)]=torch.randn(hidden_size,requires_grad=True)\n",
        "            globals()['Wh{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            \n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                hidden_state_t = output_t\n",
        "                output_t1 = torch.tanh(torch.matmul(W1,hidden_state_t) + torch.matmul(Wh1,hidden_state_t1) + b1)\n",
        "                hiddent_state_t1=output_t1\n",
        "                total_hidden_states.append(output_t1)\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def RNN_multi(inputs,hidden_state_t,num_layers):\n",
        "        for k in range(1,num_layers):\n",
        "            globals()['W{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            globals()['b{}'.format(k)]=torch.randn(hidden_size,requires_grad=True)\n",
        "            globals()['Wh{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            \n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                hidden_state_t = output_t\n",
        "                output_t1 = torch.tanh(torch.matmul(W1,hidden_state_t) + torch.matmul(Wh1,hidden_state_t1) + b1)\n",
        "                hiddent_state_t1=output_t1\n",
        "                for m in range(2,num_layers):\n",
        "                    globals()['output_t{}'.format(m)] = torch.tanh(torch.matmul(globals()['W{}'.format(m)],globals()['hidden_state_t{}'.format(m)]) + torch.matmul(globals()['Wh{}'.format(m)],globals()['hidden_state_t{}'.format(m)]) + globals()['b{}'.format(m)])\n",
        "                    globals()['hidden_state_t{}'.format(m)] = globals()['output_t{}'.format(m)]\n",
        "                total_hidden_states.append(globals()['output_t{}'.format(num_layers-1)])\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def fc(total_state):\n",
        "        linear=torch.matmul(total_state,w_fc)+b_fc\n",
        "        return linear\n",
        "    if num_layers == 1:\n",
        "        v=fc(RNN(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "    elif num_layers== 2 :\n",
        "        v=fc(RNN_two(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "    else:\n",
        "        v=fc(RNN_multi(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "RNNmodel(inputs,hidden_state_t,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QerjedRRi4in",
        "outputId": "9abf228b-5f9d-442f-d1e5-5bf9c8fd63e2"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 3.0253,  2.0219, -0.1979,  0.2563,  2.1793],\n",
              "         [-0.9591,  5.2763,  4.1475,  7.7717, -1.0933],\n",
              "         [ 0.6286,  0.5239,  2.2686,  1.4442,  4.6062],\n",
              "         [ 1.9244, -1.0175, -3.4309, -1.9974,  1.3570],\n",
              "         [ 4.1051,  3.2610,  0.7107,  0.9630,  2.9602]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters=[Wx,Wh,b,b_fc,w_fc]\n",
        "def make_parameters(num_layers):\n",
        "    for k in range(1,num_layers):\n",
        "            parameters.append(globals()['W{}'.format(k)])\n",
        "            parameters.append(globals()['b{}'.format(k)])\n",
        "            parameters.append(globals()['Wh{}'.format(k)])\n",
        "            parameters.append(globals()['hidden_state_t{}'.format(k)])\n",
        "make_parameters(num_layers)\n"
      ],
      "metadata": {
        "id": "3135wH32VtYO"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(parameters, learning_rate)"
      ],
      "metadata": {
        "id": "MAaEYC05duwo"
      },
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    total_hidden_state=[]\n",
        "    total_hidden_states=[]\n",
        "    optimizer.zero_grad()\n",
        "    outputs=RNNmodel(inputs,hidden_state_t,num_layers)\n",
        "    loss = criterion(outputs.view(-1,output_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "    \n",
        "    result = outputs.data.numpy().argmax(axis=2) \n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrk95TAAjpKE",
        "outputId": "9900dedd-936e-412a-a750-6680b2118224"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  2.5676021575927734 prediction:  [[2 1 1 4 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "1 loss:  1.328209638595581 prediction:  [[4 0 1 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "2 loss:  3.414651393890381 prediction:  [[4 3 1 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "3 loss:  2.8375041484832764 prediction:  [[4 0 3 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "4 loss:  3.963907241821289 prediction:  [[3 1 1 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "5 loss:  2.9066600799560547 prediction:  [[4 0 0 3 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "6 loss:  3.0715537071228027 prediction:  [[3 3 0 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "7 loss:  1.9304507970809937 prediction:  [[1 1 1 1 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "8 loss:  2.9184157848358154 prediction:  [[3 0 0 4 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "9 loss:  2.603450298309326 prediction:  [[3 3 3 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "10 loss:  2.7019143104553223 prediction:  [[3 2 4 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "11 loss:  2.3502442836761475 prediction:  [[3 2 1 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "12 loss:  1.6351802349090576 prediction:  [[1 4 2 3 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "13 loss:  2.121860980987549 prediction:  [[0 3 3 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "14 loss:  1.8419673442840576 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "15 loss:  2.717801809310913 prediction:  [[0 4 2 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "16 loss:  3.150871753692627 prediction:  [[2 2 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "17 loss:  2.315678358078003 prediction:  [[1 1 0 4 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "18 loss:  1.8972833156585693 prediction:  [[0 1 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "19 loss:  1.6279804706573486 prediction:  [[4 0 2 2 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "20 loss:  2.3588709831237793 prediction:  [[4 3 0 0 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "21 loss:  1.5953185558319092 prediction:  [[4 1 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "22 loss:  1.1423752307891846 prediction:  [[4 2 2 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "23 loss:  2.395127058029175 prediction:  [[3 1 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "24 loss:  1.8400523662567139 prediction:  [[3 3 3 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "25 loss:  1.8518778085708618 prediction:  [[4 4 1 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "26 loss:  1.6064409017562866 prediction:  [[2 2 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "27 loss:  1.7490781545639038 prediction:  [[0 4 3 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "28 loss:  1.7689616680145264 prediction:  [[2 2 2 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "29 loss:  1.9388377666473389 prediction:  [[4 4 3 4 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "30 loss:  1.6974445581436157 prediction:  [[4 3 2 2 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "31 loss:  1.939244270324707 prediction:  [[4 1 4 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "32 loss:  1.57793390750885 prediction:  [[4 2 4 2 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "33 loss:  2.107985496520996 prediction:  [[3 4 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "34 loss:  1.651360273361206 prediction:  [[1 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "35 loss:  1.8294503688812256 prediction:  [[3 2 3 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "36 loss:  1.714679479598999 prediction:  [[0 2 2 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "37 loss:  2.1646227836608887 prediction:  [[3 0 0 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "38 loss:  1.8908370733261108 prediction:  [[2 2 2 2 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "39 loss:  2.1893656253814697 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "40 loss:  1.2855072021484375 prediction:  [[0 0 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "41 loss:  1.6095364093780518 prediction:  [[0 1 1 2 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "42 loss:  1.47842538356781 prediction:  [[0 4 0 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "43 loss:  2.2381539344787598 prediction:  [[1 4 4 4 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "44 loss:  1.7354835271835327 prediction:  [[2 2 2 0 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "45 loss:  1.7333170175552368 prediction:  [[1 1 1 0 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "46 loss:  1.8490607738494873 prediction:  [[3 3 2 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "47 loss:  1.8300907611846924 prediction:  [[3 3 0 1 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "48 loss:  1.4976856708526611 prediction:  [[0 1 2 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "49 loss:  1.699654221534729 prediction:  [[0 0 0 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "50 loss:  2.0074996948242188 prediction:  [[3 3 1 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "51 loss:  1.5727782249450684 prediction:  [[0 0 0 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "52 loss:  1.678519606590271 prediction:  [[0 1 3 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "53 loss:  1.601872205734253 prediction:  [[3 0 4 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "54 loss:  1.632014513015747 prediction:  [[1 1 1 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "55 loss:  1.8008737564086914 prediction:  [[4 0 4 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "56 loss:  1.6834828853607178 prediction:  [[4 4 3 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "57 loss:  1.828574776649475 prediction:  [[4 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "58 loss:  1.4626522064208984 prediction:  [[3 1 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "59 loss:  1.7172006368637085 prediction:  [[3 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "60 loss:  1.8985427618026733 prediction:  [[3 3 3 1 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "61 loss:  1.369089961051941 prediction:  [[4 4 3 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "62 loss:  1.4649930000305176 prediction:  [[3 1 1 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "63 loss:  1.8714252710342407 prediction:  [[0 0 0 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "64 loss:  1.7633295059204102 prediction:  [[3 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "65 loss:  1.7473974227905273 prediction:  [[0 4 4 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "66 loss:  1.734439492225647 prediction:  [[3 0 0 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "67 loss:  1.7208465337753296 prediction:  [[1 0 0 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "68 loss:  1.5234874486923218 prediction:  [[0 1 1 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "69 loss:  1.4141498804092407 prediction:  [[4 4 1 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "70 loss:  1.5570398569107056 prediction:  [[0 0 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "71 loss:  2.4977023601531982 prediction:  [[1 0 4 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "72 loss:  1.9703229665756226 prediction:  [[0 4 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "73 loss:  1.8965492248535156 prediction:  [[2 2 1 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "74 loss:  1.6405627727508545 prediction:  [[2 1 2 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "75 loss:  1.611797571182251 prediction:  [[0 3 3 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "76 loss:  1.8099689483642578 prediction:  [[1 1 0 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "77 loss:  1.7508535385131836 prediction:  [[0 0 0 0 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "78 loss:  1.7247270345687866 prediction:  [[4 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "79 loss:  1.2423666715621948 prediction:  [[0 0 2 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "80 loss:  1.7708438634872437 prediction:  [[2 4 4 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "81 loss:  1.7914440631866455 prediction:  [[0 0 3 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "82 loss:  2.0471584796905518 prediction:  [[1 4 4 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "83 loss:  1.720452904701233 prediction:  [[4 0 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "84 loss:  1.9573307037353516 prediction:  [[2 1 3 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "85 loss:  1.6194183826446533 prediction:  [[4 4 2 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "86 loss:  1.7603744268417358 prediction:  [[3 3 2 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "87 loss:  1.6597259044647217 prediction:  [[1 4 4 1 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "88 loss:  1.842276930809021 prediction:  [[1 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "89 loss:  1.5474082231521606 prediction:  [[4 4 1 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "90 loss:  1.775162935256958 prediction:  [[3 3 1 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "91 loss:  2.015167236328125 prediction:  [[3 3 3 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "92 loss:  2.008052110671997 prediction:  [[4 4 1 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "93 loss:  1.7069454193115234 prediction:  [[1 1 3 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "94 loss:  1.7429378032684326 prediction:  [[4 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "95 loss:  1.5499542951583862 prediction:  [[2 2 2 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "96 loss:  1.7257347106933594 prediction:  [[0 0 0 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "97 loss:  1.5269635915756226 prediction:  [[2 2 2 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "98 loss:  1.7430298328399658 prediction:  [[1 2 1 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "99 loss:  1.475325584411621 prediction:  [[1 1 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7uZIRBAaJdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}