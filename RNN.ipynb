{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7xHibg3cNoned8hIV1Rex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanbuck30/neural-layer/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "URCoh7ZY4l0G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 5 # 시점의 수. NLP에서는 보통 문장의 길이\n",
        "input_size = 3 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원\n",
        "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량\n",
        "output_size=5\n",
        "batch_size=1\n",
        "inputs = torch.randn(batch_size,timesteps,input_size)\n",
        "hidden_state_t=torch.zeros(hidden_size,requires_grad=True)\n",
        "num_layers=2\n"
      ],
      "metadata": {
        "id": "CTkE6oG449h9"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wx = torch.randn(hidden_size, input_size,requires_grad=True) \n",
        "Wh = torch.randn(hidden_size, hidden_size,requires_grad=True)\n",
        "b = torch.randn(hidden_size,requires_grad=True)\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "jQ5UYob-61bi"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Wx.shape)\n",
        "print(Wh.shape)\n",
        "print(b.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0vcWbbQ7XW2",
        "outputId": "743a995d-fe29-4fa2-824e-2692751d40a6"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3])\n",
            "torch.Size([8, 8])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJB-9u6W3aOJ",
        "outputId": "3ce97ff7-8764-40e9-9c13-2c8441bedffe"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y=torch.LongTensor([[0,1,2,3,4]])\n",
        "y_data=[Y]\n",
        "Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srZJhB-ck4ev",
        "outputId": "0cd80dad-4765-46c0-974e-cb402f92dce4"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b_fc=torch.randn(output_size,requires_grad=True)\n",
        "w_fc=torch.randn(hidden_size, output_size,requires_grad=True)"
      ],
      "metadata": {
        "id": "y5usjXzofPhm"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_hidden_state=[]\n",
        "total_hidden_states=[]\n",
        "def RNNmodel(inputs,hidden_state_t,num_layers):\n",
        "    def RNN(inputs,hidden_state_t,num_layers):\n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                total_hidden_states.append(output_t) \n",
        "                hidden_state_t = output_t\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def RNN_two(inputs,hidden_state_t,num_layers):\n",
        "        for k in range(1,num_layers):\n",
        "            globals()['W{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            globals()['b{}'.format(k)]=torch.randn(hidden_size,requires_grad=True)\n",
        "            globals()['Wh{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            \n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                hidden_state_t = output_t\n",
        "                output_t1 = torch.tanh(torch.matmul(W1,hidden_state_t) + torch.matmul(Wh1,hidden_state_t1) + b1)\n",
        "                hiddent_state_t1=output_t1\n",
        "                total_hidden_states.append(output_t1)\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def RNN_multi(inputs,hidden_state_t,num_layers):\n",
        "        for k in range(1,num_layers):\n",
        "            globals()['W{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            globals()['b{}'.format(k)]=torch.randn(hidden_size,requires_grad=True)\n",
        "            globals()['Wh{}'.format(k)]=torch.randn(hidden_size,hidden_size,requires_grad=True)\n",
        "            \n",
        "        for i in inputs:\n",
        "            for input_t in i: \n",
        "                output_t = torch.tanh(torch.matmul(Wx,input_t) + torch.matmul(Wh,hidden_state_t) + b)\n",
        "                hidden_state_t = output_t\n",
        "                output_t1 = torch.tanh(torch.matmul(W1,hidden_state_t) + torch.matmul(Wh1,hidden_state_t1) + b1)\n",
        "                hiddent_state_t1=output_t1\n",
        "                for m in range(2,num_layers):\n",
        "                    globals()['output_t{}'.format(m)] = torch.tanh(torch.matmul(globals()['W{}'.format(m)],globals()['hidden_state_t{}'.format(m)]) + torch.matmul(globals()['Wh{}'.format(m)],globals()['hidden_state_t{}'.format(m)]) + globals()['b{}'.format(m)])\n",
        "                    globals()['hidden_state_t{}'.format(m)] = globals()['output_t{}'.format(m)]\n",
        "                total_hidden_states.append(globals()['output_t{}'.format(num_layers)])\n",
        "            total_state = torch.stack(total_hidden_states,dim=0)\n",
        "            total_hidden_state.append(total_state)\n",
        "        total_state=torch.stack(total_hidden_state,dim=0)\n",
        "        return total_state\n",
        "    def fc(total_state):\n",
        "        linear=torch.matmul(total_state,w_fc)+b_fc\n",
        "        return linear\n",
        "    if num_layers == 1:\n",
        "        v=fc(RNN(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "    elif num_layers== 2 :\n",
        "        v=fc(RNN_two(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "    else:\n",
        "        v=fc(RNN_multi(inputs,hidden_state_t,num_layers))\n",
        "        return v\n",
        "RNNmodel(inputs,hidden_state_t,3).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QerjedRRi4in",
        "outputId": "3301f174-528c-40ae-fcc2-cd19388a0ef1"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters=[Wx,Wh,b,b_fc,w_fc]\n",
        "def make_parameters(num_layers):\n",
        "    for k in range(1,num_layers):\n",
        "            parameters.append(globals()['W{}'.format(k)])\n",
        "            parameters.append(globals()['b{}'.format(k)])\n",
        "            parameters.append(globals()['Wh{}'.format(k)])\n",
        "            parameters.append(globals()['hidden_state_t{}'.format(k)])\n",
        "make_parameters(num_layers)\n"
      ],
      "metadata": {
        "id": "3135wH32VtYO"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(parameters, learning_rate)"
      ],
      "metadata": {
        "id": "MAaEYC05duwo"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    total_hidden_state=[]\n",
        "    total_hidden_states=[]\n",
        "    optimizer.zero_grad()\n",
        "    outputs=RNNmodel(inputs,hidden_state_t,num_layers)\n",
        "    loss = criterion(outputs.view(-1,output_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "    \n",
        "    result = outputs.data.numpy().argmax(axis=2) \n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrk95TAAjpKE",
        "outputId": "f6e128da-97db-4779-f028-1098ff3814ef"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  3.44317364692688 prediction:  [[1 2 2 2 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "1 loss:  3.5672614574432373 prediction:  [[2 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "2 loss:  2.692159652709961 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "3 loss:  2.6726300716400146 prediction:  [[0 0 4 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "4 loss:  2.6899306774139404 prediction:  [[2 2 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "5 loss:  2.3069047927856445 prediction:  [[4 2 4 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "6 loss:  2.1264569759368896 prediction:  [[4 4 2 2 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "7 loss:  1.7154020071029663 prediction:  [[3 2 2 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "8 loss:  2.5024526119232178 prediction:  [[3 4 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "9 loss:  1.716629981994629 prediction:  [[3 4 2 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "10 loss:  1.8698012828826904 prediction:  [[3 3 3 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "11 loss:  3.0822365283966064 prediction:  [[2 2 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "12 loss:  2.068559169769287 prediction:  [[3 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "13 loss:  2.2595131397247314 prediction:  [[1 1 3 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "14 loss:  2.2922961711883545 prediction:  [[4 4 4 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "15 loss:  2.996358871459961 prediction:  [[4 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "16 loss:  2.539048671722412 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "17 loss:  2.320322036743164 prediction:  [[1 1 4 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "18 loss:  2.991725444793701 prediction:  [[4 4 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "19 loss:  2.2775888442993164 prediction:  [[2 3 2 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "20 loss:  1.791272521018982 prediction:  [[4 3 4 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "21 loss:  1.8948217630386353 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "22 loss:  2.172929048538208 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "23 loss:  2.245060682296753 prediction:  [[1 1 1 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "24 loss:  1.5573146343231201 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "25 loss:  1.6390583515167236 prediction:  [[4 1 1 4 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "26 loss:  1.8368756771087646 prediction:  [[1 1 1 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "27 loss:  1.976288080215454 prediction:  [[2 2 1 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "28 loss:  2.1597483158111572 prediction:  [[2 0 0 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "29 loss:  1.8413528203964233 prediction:  [[1 1 2 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "30 loss:  1.4581667184829712 prediction:  [[0 0 3 4 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "31 loss:  1.6864597797393799 prediction:  [[2 1 1 2 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "32 loss:  1.8490135669708252 prediction:  [[1 4 3 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "33 loss:  1.6128218173980713 prediction:  [[1 1 1 0 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "34 loss:  1.366088628768921 prediction:  [[0 0 2 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "35 loss:  1.8054273128509521 prediction:  [[0 1 0 0 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "36 loss:  2.1318042278289795 prediction:  [[2 2 3 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "37 loss:  2.239081621170044 prediction:  [[3 3 3 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "38 loss:  2.132070541381836 prediction:  [[2 1 1 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "39 loss:  1.9003307819366455 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "40 loss:  1.740138053894043 prediction:  [[2 4 2 2 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "41 loss:  1.8954370021820068 prediction:  [[3 3 1 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "42 loss:  1.336434245109558 prediction:  [[0 0 2 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "43 loss:  1.6620228290557861 prediction:  [[4 4 2 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "44 loss:  1.6186325550079346 prediction:  [[3 2 1 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "45 loss:  1.6516964435577393 prediction:  [[3 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "46 loss:  1.6569604873657227 prediction:  [[0 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "47 loss:  1.7487080097198486 prediction:  [[2 4 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "48 loss:  1.6461913585662842 prediction:  [[4 1 1 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "49 loss:  1.5253592729568481 prediction:  [[0 0 3 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "50 loss:  1.459594488143921 prediction:  [[4 4 4 2 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "51 loss:  1.9739515781402588 prediction:  [[2 2 4 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "52 loss:  1.31090247631073 prediction:  [[0 4 3 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "53 loss:  1.6542692184448242 prediction:  [[2 0 3 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "54 loss:  1.7063325643539429 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "55 loss:  1.388256311416626 prediction:  [[0 4 3 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "56 loss:  1.5847856998443604 prediction:  [[4 0 0 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "57 loss:  1.8602889776229858 prediction:  [[0 1 0 0 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "58 loss:  1.4378973245620728 prediction:  [[4 4 0 3 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "59 loss:  1.6491577625274658 prediction:  [[0 0 0 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "60 loss:  2.0283591747283936 prediction:  [[1 0 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "61 loss:  1.814307451248169 prediction:  [[3 1 1 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "62 loss:  1.4847204685211182 prediction:  [[0 2 3 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "63 loss:  1.7235820293426514 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "64 loss:  1.5485700368881226 prediction:  [[1 2 3 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "65 loss:  1.9186742305755615 prediction:  [[2 0 0 0 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "66 loss:  2.020994186401367 prediction:  [[3 3 3 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "67 loss:  2.173307180404663 prediction:  [[1 4 4 4 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "68 loss:  1.703411340713501 prediction:  [[0 3 0 3 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "69 loss:  1.7814304828643799 prediction:  [[3 3 1 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "70 loss:  1.4733068943023682 prediction:  [[0 2 2 2 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "71 loss:  1.6637130975723267 prediction:  [[3 1 3 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "72 loss:  1.6498870849609375 prediction:  [[2 3 2 2 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "73 loss:  1.6408157348632812 prediction:  [[2 2 2 2 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "74 loss:  1.5708577632904053 prediction:  [[1 1 1 0 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "75 loss:  1.8748219013214111 prediction:  [[2 2 2 2 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "76 loss:  1.6288070678710938 prediction:  [[2 1 2 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "77 loss:  1.8624703884124756 prediction:  [[4 0 4 3 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "78 loss:  1.689884901046753 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "79 loss:  1.8094542026519775 prediction:  [[2 2 2 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "80 loss:  1.7423585653305054 prediction:  [[2 2 2 3 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "81 loss:  1.6122477054595947 prediction:  [[4 4 3 0 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "82 loss:  1.8542892932891846 prediction:  [[1 1 1 1 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "83 loss:  1.5757347345352173 prediction:  [[4 4 3 3 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "84 loss:  1.5950090885162354 prediction:  [[0 0 0 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "85 loss:  1.4314420223236084 prediction:  [[3 1 3 3 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "86 loss:  1.862596869468689 prediction:  [[2 1 1 1 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "87 loss:  1.809614896774292 prediction:  [[1 1 1 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "88 loss:  1.9550228118896484 prediction:  [[3 1 3 1 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "89 loss:  1.6379903554916382 prediction:  [[1 1 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "90 loss:  1.698280692100525 prediction:  [[1 1 3 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "91 loss:  1.7114957571029663 prediction:  [[1 1 1 1 1]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "92 loss:  1.7048012018203735 prediction:  [[0 0 0 0 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "93 loss:  1.6589031219482422 prediction:  [[4 4 1 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "94 loss:  1.8553879261016846 prediction:  [[3 3 4 3 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "95 loss:  1.6501401662826538 prediction:  [[1 0 1 1 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "96 loss:  1.6994361877441406 prediction:  [[3 3 0 0 3]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "97 loss:  1.9617984294891357 prediction:  [[4 4 4 4 4]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "98 loss:  1.5925092697143555 prediction:  [[0 0 2 2 0]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n",
            "99 loss:  1.69327712059021 prediction:  [[4 1 4 4 2]] true Y:  [tensor([[0, 1, 2, 3, 4]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7uZIRBAaJdY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}